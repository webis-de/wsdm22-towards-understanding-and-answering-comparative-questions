{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import ast\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May be needed for experiments on GPU. The experiments can be run on CPU, too \n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"successful allowed memory to grow\")\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of usage are below\n",
    "\n",
    "def run_crossvalidation(model, questions, X, y, path_splits, save_splits=False, make_plot=True, thresholds, \n",
    "                        nr_splits=10, early_stop=True, plot_label=\"\", is_keras=False, batch_size=5, return_prob_df=False):\n",
    "    y_cv_true = []\n",
    "    y_cv_predict = []\n",
    "    question_cv = []\n",
    "    kf = StratifiedKFold(n_splits=nr_splits)\n",
    "    split = 1\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        y_cv_true.extend(y_test)\n",
    "        question_cv.extend(questions[test_index])\n",
    "        if not is_keras:\n",
    "            temp_model = clone(model)\n",
    "            temp_model.fit(X_train, y_train)\n",
    "            predictions = temp_model.predict_proba(X_test)\n",
    "            predictions = predictions[:,1]\n",
    "        #keras models:\n",
    "        else:\n",
    "            temp_model = tf.keras.models.clone_model(model)\n",
    "            temp_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[TruePositives()])\n",
    "            es = EarlyStopping(monitor='loss', mode='min', verbose=0)\n",
    "            temp_model.fit(X_train, y_train, epochs=100, batch_size=batch_size, verbose=0, callbacks=[es])\n",
    "            predictions = temp_model.predict(X_test, batch_size=batch_size)\n",
    "            \n",
    "        y_cv_predict.extend(predictions)\n",
    "\n",
    "        if save_splits:\n",
    "            questions_cv = questions[test_index]\n",
    "            df_out = pd.DataFrame({\"comp\": y_test, \"clean\": questions_cv, \"prediction\": predictions})\n",
    "            df_out.to_csv(path_splits + \"split_{}\".format(str(split)) + \".tsv\", index=False, sep=\"\\t\")\n",
    "        split += 1\n",
    "    \n",
    "    predictions = np.array(y_cv_predict)\n",
    "    y_true = np.array(y_cv_true )\n",
    "    \n",
    "\n",
    "    if make_plot:\n",
    "        precision_scores, recall_scores = list(), list()\n",
    "        for threshold in tqdm(thresholds):\n",
    "            prob_preds = np.where(predictions>=threshold, 1, 0)\n",
    "            temp_classification_report = classification_report(y_true=y_true, y_pred=prob_preds, output_dict=True)['1']\n",
    "            precision = round(temp_classification_report['precision'], 3)\n",
    "            precision_scores.append(precision)                      \n",
    "            recall_scores.append(round(temp_classification_report['recall'], 3))\n",
    "            if early_stop:\n",
    "                if precision < 0.90:\n",
    "                    break\n",
    "\n",
    "        l = [item for item in zip(precision_scores, recall_scores, list(thresholds)) if item[0] != 0]\n",
    "\n",
    "        plt.plot([i[1] for i in l], [i[0] for i in l], marker='.', label=plot_label)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        #show the legend\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        #show the plot\n",
    "        plt.show()\n",
    "\n",
    "        l1 = [i for i in zip(precision_scores, recall_scores, list(thresholds)) if i[0]==1]\n",
    "        try:\n",
    "            print(\"Comp. questions: max Prec. {:.3f} with Rec. {:.3f} at thresh. {:.6f}\".format(l1[-1][0], l1[-1][1], l1[-1][2]))\n",
    "        except:\n",
    "            print(\"Model doesn't reach precision of 1.00\")\n",
    "        try:\n",
    "            l3 = [item for item in l if 0.95 < item[0] < 1]\n",
    "            print(\"Comp. questions: max Prec. {:.3f} with Rec. {:.3f} at thresh. {:.6f}\".format(l3[-1][0], l3[-1][1], l3[-1][2]))\n",
    "            print(\"F1: {:.3f}\".format(2*l3[-1][0]*l3[-1][1]/(l3[-1][0] + l3[-1][1])))\n",
    "        except:\n",
    "            print(\"Model doesn't reach precision of 0.95\")\n",
    "        try:\n",
    "            l2 = [item for item in l if 0.90 < item[0] < 1]\n",
    "            print(\"Comp. questions: max Prec. {:.3f} with Rec. {:.3f} at thresh. {:.6f}\".format(l2[-1][0], l2[-1][1], l2[-1][2]))\n",
    "            print(\"F1: {:.3f}\".format(2*l2[-1][0]*l2[-1][1]/(l2[-1][0] + l2[-1][1])))\n",
    "        except:print(\"Model doesn't reach precision of 0.90\")\n",
    "\n",
    "    if return_prob_df:\n",
    "        return  pd.DataFrame({\"clean\": question_cv, \"prob\": y_cv_predict})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This neural network is used for the embeddings by the \"base\" transformer models\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import TruePositives, Precision\n",
    "\n",
    "tf.random.set_seed(2)\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(768,), activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[TruePositives()]) #metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This neural network is used for the embeddings by the \"large\" transformer models\n",
    "\n",
    "tf.random.set_seed(2)\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(512, input_shape=(1024,), activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model2.add(Dense(256, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[TruePositives()]) #metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data, e.g., very hard questions (after logistic regression)\n",
    "very_hard = pd.read_csv(\"very_hard.tsv\", sep=\"\\t\")[[\"comp\", \"clean\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute question representations using pre-trained transformer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "questions = very_hard[\"clean\"] # questions cleansed from punctuation are used\n",
    "\n",
    "# you can specify any model available in the transformers library\n",
    "# options used in the paper include roberta-base, roberta-large, sentence-transformers/bert-large-nli-mean-tokens, facebook/bart-large-cnn\n",
    "# you should specify the same model name, both in model and tokenizer\n",
    "\n",
    "feature_extraction = pipeline('feature-extraction', model=\"roberta-base\", tokenizer=\"roberta-base\", device=0) # device=-1 for CPU, device=0 for GPU\n",
    "\n",
    "# both the CLS-token emebeddings (only), and the mean of all tokens can be used\n",
    "X_cls, X_mean = list(), list()\n",
    "\n",
    "for question in tqdm(questions):\n",
    "    features = feature_extraction(question)\n",
    "    cls = features[0][0]\n",
    "    features = np.mean(features[0], axis=0)\n",
    "    X_mean.append(features)\n",
    "    X_cls.append(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the CLS-token emebeddings\n",
    "\n",
    "X = np.array(X_cls)\n",
    "X = np.stack(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the mean of all token embeddings\n",
    "\n",
    "X = np.array(X_mean)\n",
    "X = np.stack(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example to run classification experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = very_hard[\"comp\"] # true labels\n",
    "\n",
    "roberta_base_cls_very_hard = run_crossvalidation(model = model, #\"model\" when transformer-base is used, \"model2\" when large \n",
    "questions=very_hard[\"clean\"], #cleaned questions\n",
    "X=X, #representations: either for CLS-token emebeddings or the mean of all token embeddings\n",
    "y=y, # true labels\n",
    "path_splits='', #specifies the path to save the results for each split of the cross-validation (if save_splits=True)\n",
    "save_splits=False, #set True to save the results for each split of the cross-validation\n",
    "make_plot=True, #True to plot the precision-recall curve\n",
    "thresholds=np.arange(1,0, -0.001), #range of the classifier's probability thresholds for plotting the precision-recall curve\n",
    "nr_splits=10, #number of CV splipts\n",
    "early_stop=True,\n",
    "plot_label=\"feed forward NN\",\n",
    "is_keras=True,\n",
    "batch_size=5,\n",
    "return_prob_df=True) #returns the results as a dataframe that contains classifier's probabilities\n",
    "\n",
    "roberta_base_cls_very_hard.to_csv('', sep=\"\\t\") #returnd dataframe can be saved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
