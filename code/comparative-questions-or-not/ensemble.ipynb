{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook implements the ensemble classifier that identifies comparative questions. The pre-calculated results (logistic regression, transformer-based) are in the results.zip directory that should be unzipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\", sep=\"\\t\") # specifies the path to data_full.tsv in Webis-CompQuestions-22/comparative-questions-or-not\n",
    "df[\"pos\"] = df[\"pos\"].apply(eval)\n",
    "df[\"lemma\"] = df[\"lemma\"].apply(eval)\n",
    "df[\"tokenized\"] = df[\"clean\"].apply(lambda x: re.split(\" \", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_and(inp, regexs):\n",
    "    for rex in regexs:\n",
    "        if rex.match(inp) is None:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def re_or(inp, regexs):\n",
    "    for rex in regexs:\n",
    "        if rex.match(inp) is not None:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def re_one(inp, regex):\n",
    "    if regex.match(inp) is not None:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_interrogative(inp):\n",
    "    interrogratives = [\"who\", \"which\", \"what\", \"where\", \"when\", \"how\", \"why\"]\n",
    "    tokens = word_tokenize(inp[\"clean\"])\n",
    "    for i in interrogratives:\n",
    "        if i in tokens:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_singular_interrogative(inp):\n",
    "    interrogratives = [\"who\", \"which\", \"what\", \"where\"]\n",
    "    tokens = word_tokenize(inp[\"clean\"])\n",
    "    for i in interrogratives:\n",
    "        if i in tokens:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def pos_match(inp, pos_tags):\n",
    "    for p in inp:\n",
    "        if p in pos_tags:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_in_seq(arr, q):\n",
    "    if q[0] not in arr:\n",
    "        return -1\n",
    "    pos = arr.index(q[0])\n",
    "    for it, x in enumerate(q[1:]):\n",
    "        if x != arr[it + pos +1]:\n",
    "            return -1\n",
    "    return pos + len(q)\n",
    "\n",
    "def clean_df():\n",
    "    if \"pred\" in df.columns:\n",
    "        df[\"pred\"] = False\n",
    "    if \"neg_prediction\" in df.columns:\n",
    "        df[\"neg_prediction\"] = False\n",
    "    for col in df.columns:\n",
    "        if \"Unnamed: 0\" in col:\n",
    "            del df[col]\n",
    "        if \"pos_rule\" in col:\n",
    "            del df[col]\n",
    "        if \"neg_rule\" in col:\n",
    "            del df[col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pred\"] = False\n",
    "df[\"truth\"] = df[\"comp\"].apply(lambda x: True if x==1.0 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex needed for negative rules:\n",
    "rex_neg_rule_0 = re.compile(r'((when|how (long|many|much))\\b.*)')\n",
    "\n",
    "\n",
    "def neg_rule_0(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    regexs = [rex_neg_rule_0]\n",
    "    return re_and(question, regexs)# and not((\"JJS\" in pos) or (\"RBS\" in pos))\n",
    "\n",
    "evil_words = [\"lyrics\", \"wrote\", \"mean\", \"covered\", \"cast\", \"played\", \"season\", \"episode\", \"award\", \"sing\", \"sang\", \"song\", \"album\", \"movie\"]\n",
    "\n",
    "def neg_rule_1(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    for evil in evil_words:\n",
    "        if evil in question:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def neg_rule_2(inp):\n",
    "    token = inp['tokenized']\n",
    "    if len(token) < 4:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex needed for pos rules\n",
    "rex_pos_rule_2_1 = re.compile(r'(.*difference between.*)')\n",
    "rex_pos_rule_2_2 = re.compile(r'(.*compare to.*)')\n",
    "rex_pos_rule_3 = re.compile(r'((which) (are|is|\\'s|s)\\b.*\\b(a|an|the)\\b.*)')\n",
    "rex_pos_rule_4_1 = re.compile(r'(what are( some)? good\\b.*)')\n",
    "rex_pos_rule_4_2 = re.compile(r'(.*\\byour favorite\\b.*)')\n",
    "rex_pos_rule_5 = re.compile(r'(.*(who was (the|a|an).*))')\n",
    "rex_pos_rule_6 = re.compile(r'((which).*(should i))')\n",
    "rex_pos_rule_9_1 = re.compile(r'(.*\\b(or|and|from|between|vs|and|versus)\\b.*)')\n",
    "rex_pos_rule_9_2 = re.compile(r'(.*\\b(distinguish|differ|differentiate|differences|strengths|weaknesses)\\b.*)')\n",
    "rex_pos_rule_9_3 = re.compile(r'(.*(the differen(ce|t) between).*)')\n",
    "rex_pos_rule_10 = re.compile(r'(.*(the (best)).*)')\n",
    "rex_pos_rule_11 = re.compile(r'.*\\b(is)\\b.*')\n",
    "rex_pos_rule_13_1 = re.compile(r'(.*are\\b.*and\\b.*same.*)')\n",
    "rex_pos_rule_13_2 = re.compile(r'(.*same.*\\bas\\b.*)')\n",
    "rex_pos_rule_13_3 = re.compile(r'(.*are|is.*)')\n",
    "\n",
    "def pos_rule_1(row):\n",
    "    if row[\"neg_prediction\"]:\n",
    "        return False\n",
    "    tokens = row[\"tokenized\"]\n",
    "    try:\n",
    "        pos_JJ = max(\n",
    "            find_in_seq(tokens, [\"what\", \"is\"]), \n",
    "            find_in_seq(tokens, [\"what\", \"are\"]), \n",
    "            find_in_seq(tokens, [\"what\", \"s\"]),\n",
    "            find_in_seq(tokens, [\"what\", \"'s\"]),\n",
    "            find_in_seq(tokens, [\"which\", \"is\"]), \n",
    "            find_in_seq(tokens, [\"which\", \"are\"]), \n",
    "            find_in_seq(tokens, [\"which\", \"s\"]),\n",
    "            find_in_seq(tokens, [\"which\", \"'s\"]),\n",
    "            find_in_seq(tokens, [\"is\", \"the\"]), \n",
    "            find_in_seq(tokens, [\"are\", \"the\"]), \n",
    "            find_in_seq(tokens, [\"s\", \"the\"]),\n",
    "            find_in_seq(tokens, [\"'s\", \"the\"])\n",
    "\n",
    "        )\n",
    "        if pos_JJ < 0:\n",
    "            return False\n",
    "        if tokens[pos_JJ] in [\"difference\", \"differences\", \"pros\", \"good\", \"advantages\", \"better\", \"similarities\"]:\n",
    "            return True\n",
    "    except:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def pos_rule_2(row):\n",
    "    tokenized = row[\"tokenized\"]\n",
    "    pos = row[\"pos\"]\n",
    "    question = row[\"clean\"]\n",
    "    return re_or(question, [rex_pos_rule_2_1, rex_pos_rule_2_2]) and not row[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_3(row):\n",
    "    question = row[\"clean\"]\n",
    "    pos = row[\"pos\"]\n",
    "    return re_one(question, rex_pos_rule_3) and pos_match(pos, [\"JJS\", \"RBS\", \"JJR\", \"RBR\"]) and not row[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_4(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    return re_or(question, [rex_pos_rule_4_1, rex_pos_rule_4_2]) and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_5(inp):\n",
    "    pos = inp[\"pos\"]\n",
    "    tokens = inp[\"tokenized\"]\n",
    "    question = inp[\"clean\"]\n",
    "    return re_one(question, rex_pos_rule_5) and pos[3] in [\"JJS\", \"RBS\", \"RBR\", \"JJR\"] and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_6(inp):\n",
    "    pos = inp[\"pos\"]\n",
    "    tokens = inp[\"tokenized\"]\n",
    "    question = inp[\"clean\"]\n",
    "    return re_one(question, rex_pos_rule_6) and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_7(inp):\n",
    "    pos = inp[\"pos\"]\n",
    "    tokens = inp[\"tokenized\"]\n",
    "    if pos[0] in [\"JBS\", \"RBS\"]:\n",
    "        return True and not inp[\"neg_prediction\"]\n",
    "    if tokens[0] in [\"the\", \"a\", \"an\"] and pos[1] in [\"JBS\", \"RBS\"]:\n",
    "        return True and not inp[\"neg_prediction\"]\n",
    "    return False\n",
    "\n",
    "def pos_rule_8(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    if inp[\"neg_prediction\"]:\n",
    "        return False\n",
    "    tokens = inp[\"tokenized\"]\n",
    "    try:\n",
    "        pos_JJ = max(\n",
    "            find_in_seq(tokens, [\"what\", \"is\"]), \n",
    "            find_in_seq(tokens, [\"what\", \"are\"]), \n",
    "            find_in_seq(tokens, [\"what\", \"s\"]),\n",
    "            find_in_seq(tokens, [\"what\", \"'s\"]),\n",
    "            find_in_seq(tokens, [\"which\", \"is\"]), \n",
    "            find_in_seq(tokens, [\"which\", \"are\"]), \n",
    "            find_in_seq(tokens, [\"which\", \"s\"]),\n",
    "            find_in_seq(tokens, [\"which\", \"'s\"]),\n",
    "            find_in_seq(tokens, [\"is\", \"the\"]), \n",
    "            find_in_seq(tokens, [\"are\", \"the\"]), \n",
    "            find_in_seq(tokens, [\"s\", \"the\"]),\n",
    "            find_in_seq(tokens, [\"'s\", \"the\"])\n",
    "        )\n",
    "        if pos_JJ < 0:\n",
    "            return False\n",
    "        if pos[pos_JJ] in [\"RBS\", \"JJS\"]:\n",
    "            return True and not neg_rule_2(inp)\n",
    "        if tokens[pos_JJ] in [\"most\"]:\n",
    "            return True and not neg_rule_2(inp)\n",
    "    except:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def pos_rule_9(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    if \"how\" in question:\n",
    "        return False\n",
    "    return re_and(question, [rex_pos_rule_9_1, rex_pos_rule_9_2]) or re_one(question, rex_pos_rule_9_3) and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_10(inp):\n",
    "    pos = inp[\"pos\"]\n",
    "    tokens = inp[\"tokenized\"]\n",
    "    question = inp[\"clean\"]\n",
    "    if ('for the best' in question) or ('how is' in question):\n",
    "        return False\n",
    "    return re_one(question, rex_pos_rule_10) and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_11(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    return has_singular_interrogative(inp) and pos_match(pos, [\"JJS\", \"RBS\"]) and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_12(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    tokenized = inp[\"tokenized\"]\n",
    "    for it, p in enumerate(pos):\n",
    "        if p in [\"JJS\", \"RBS\"] and \"the\" in tokenized[0: it]:\n",
    "            return True  and not inp[\"neg_prediction\"]\n",
    "    return False\n",
    "\n",
    "def pos_rule_13(inp):\n",
    "    tokenized = inp[\"tokenized\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    question = inp[\"clean\"]\n",
    "    return (re_one(question, rex_pos_rule_13_1) or re_and(question, [rex_pos_rule_13_2, rex_pos_rule_13_3])) and not inp[\"neg_prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df()\n",
    "#NEGATIVE RULES##################################################################\n",
    "#if neg_prediction is True, the question is not a comparative question\n",
    "# if False it is a comparative question\n",
    "neg_rules = [\"neg_rule_0\", \"neg_rule_1\"]\n",
    "df[\"neg_rule_0\"] = df.apply(lambda x: neg_rule_0(x), axis=1)\n",
    "df[\"neg_rule_1\"] = df.apply(lambda x: neg_rule_1(x), axis=1)\n",
    "\n",
    "def make_prediction(old_prediction, prediction_rule):\n",
    "    return old_prediction or prediction_rule\n",
    "\n",
    "df[\"neg_prediction\"] = False\n",
    "for neg_rule in neg_rules:\n",
    "    df[\"neg_prediction\"] = df.apply(lambda x: make_prediction(x[\"neg_prediction\"], x[neg_rule]), axis=1)\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "#POSITIVE RULES###################################################################\n",
    "#if pred is True, question is comparative\n",
    "#if pred is False, question is not comparative\n",
    "pos_rules = [pos_rule_1, pos_rule_2, pos_rule_3, pos_rule_4, pos_rule_5, pos_rule_6, pos_rule_7, pos_rule_8, pos_rule_9, pos_rule_10, pos_rule_11, pos_rule_12, pos_rule_13]\n",
    "pos_rules_100 = [pos_rule_1, pos_rule_2, pos_rule_3, pos_rule_4, pos_rule_5, pos_rule_6, pos_rule_7, pos_rule_8, pos_rule_9, pos_rule_10]\n",
    "\n",
    "precision_per_rule = []\n",
    "recall_per_rule = []\n",
    "precision_total = []\n",
    "recall_total = []\n",
    "for pos_rule in pos_rules_100:\n",
    "    df[pos_rule.__name__] = df.apply(lambda x: pos_rule(x), axis=1)\n",
    "    df[\"pred\"] = df.apply(lambda x: make_prediction(x[\"pred\"], x[pos_rule.__name__]), axis=1)\n",
    "\n",
    "    precision_per_rule.append(precision_score(df[\"truth\"], df[pos_rule.__name__]))\n",
    "    recall_per_rule.append(recall_score(df[\"truth\"], df[pos_rule.__name__]))\n",
    "\n",
    "    precision_total.append(precision_score(df[\"truth\"], df[\"pred\"]))\n",
    "    recall_total.append(recall_score(df[\"truth\"], df[\"pred\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"comp\", \"clean\", \"pred\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all result data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_probs(split, name, path=\"./results/\"):\n",
    "    temp_df = pd.read_csv(path + split + \"/\" + name + \".tsv\", sep=\"\\t\")\n",
    "    temp_df[name] = temp_df[\"prob\"].apply(eval).apply(lambda x: x[0])\n",
    "    temp_df = temp_df[[\"clean\", name]]\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pd.read_csv(\"./results/hard/lr2_hard.tsv\", sep=\"\\t\") # the results.zip should be unzipped\n",
    "lr[\"lr\"] = lr[\"prob\"]\n",
    "\n",
    "\n",
    "# models trained on hard questions:\n",
    "roberta_base_mean_hard = load_probs(split=\"hard\", name=\"roberta_base_mean_hard\")\n",
    "\n",
    "roberta_base_cls_hard = load_probs(split=\"hard\", name=\"roberta_base_cls_hard\")\n",
    "\n",
    "roberta_large_cls_hard = load_probs(split=\"hard\", name=\"roberta_large_cls_hard\")\n",
    "\n",
    "bart_large_mean_hard = load_probs(split=\"hard\", name=\"bart_large_mean_hard\")\n",
    "\n",
    "#models trained on very hard questions:\n",
    "roberta_base_cls_very_hard = load_probs(split=\"very_hard\", name=\"roberta_base_cls_very_hard\")\n",
    "\n",
    "bart_large_mean_very_hard = load_probs(split=\"very_hard\", name=\"bart_large_mean_very_hard\")\n",
    "\n",
    "roberta_base_mean_very_hard = load_probs(split=\"very_hard\", name=\"roberta_base_mean_very_hard\")\n",
    "\n",
    "roberta_large_cls_very_hard = load_probs(split=\"very_hard\", name=\"roberta_large_cls_very_hard\")\n",
    "\n",
    "sentencebert_large_cls_very_hard = load_probs(split=\"very_hard\", name=\"sbert_large_cls_very_hard\")\n",
    "\n",
    "sentencebert_large_mean_very_hard = load_probs(split=\"very_hard\", name=\"sbert_large_mean_very_hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all(dfs):\n",
    "    for e in range(len(dfs)):\n",
    "        if e == 0:\n",
    "            df = dfs[0]\n",
    "        else:\n",
    "            df = df.merge(dfs[e], on=\"clean\", how=\"left\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = merge_all(dfs=[\n",
    "    df,\n",
    "    lr,\n",
    "    roberta_base_mean_hard,\n",
    "    roberta_base_cls_hard,\n",
    "    roberta_large_cls_hard,\n",
    "    bart_large_mean_hard,\n",
    "    roberta_base_cls_very_hard,\n",
    "    bart_large_mean_very_hard,\n",
    "    roberta_base_mean_very_hard,\n",
    "    roberta_large_cls_very_hard,\n",
    "    sentencebert_large_cls_very_hard,\n",
    "    sentencebert_large_mean_very_hard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaging the probabilities at the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"avg\"] = res.iloc[:,[True if (\"lr\" in col) or (\"hard\" in col) else False for col in res.columns]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "thresholds = np.append(np.arange(1,0.9, -0.0001), np.arange(0.9,0, -0.001))\n",
    "\n",
    "precision_scores, recall_scores = list(), list()\n",
    "predictions = res[\"avg\"]\n",
    "for threshold in tqdm(thresholds):\n",
    "    prob_preds = np.where(predictions>=threshold, 1, 0)\n",
    "    temp_classification_report = classification_report(y_true=y_true, y_pred=prob_preds, output_dict=True)['1']\n",
    "    precision = round(temp_classification_report['precision'], 3)\n",
    "    precision_scores.append(precision)                      \n",
    "    recall_scores.append(round(temp_classification_report['recall'], 3))\n",
    "\n",
    "l = [item for item in zip(precision_scores, recall_scores, list(thresholds)) if item[0] != 0]\n",
    "\n",
    "plt.plot([i[1] for i in l], [i[0] for i in l], marker='.', label=\"avg prediction\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "#show the legend\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#show the plot\n",
    "plt.show()\n",
    "\n",
    "l1 = [i for i in zip(precision_scores, recall_scores, list(thresholds)) if i[0]==1]\n",
    "try:\n",
    "    print(\"Comp. questions: max Prec. {:.3f} with Rec. {:.3f} at thresh. {:.6f}\".format(l1[-1][0], l1[-1][1], l1[-1][2]))\n",
    "except:\n",
    "    print(\"Model doesn't reach precision of 1.00\")\n",
    "try:\n",
    "    l3 = [item for item in l if 0.95 < item[0] < 1]\n",
    "    print(\"Comp. questions: max Prec. {:.3f} with Rec. {:.3f} at thresh. {:.6f}\".format(l3[-1][0], l3[-1][1], l3[-1][2]))\n",
    "    print(\"F1: {:.3f}\".format(2*l3[-1][0]*l3[-1][1]/(l3[-1][0] + l3[-1][1])))\n",
    "except:\n",
    "    print(\"Model doesn't reach precision of 0.95\")\n",
    "try:\n",
    "    l2 = [item for item in l if 0.90 < item[0] < 1]\n",
    "    print(\"Comp. questions: max Prec. {:.3f} with Rec. {:.3f} at thresh. {:.6f}\".format(l2[-1][0], l2[-1][1], l2[-1][2]))\n",
    "    print(\"F1: {:.3f}\".format(2*l2[-1][0]*l2[-1][1]/(l2[-1][0] + l2[-1][1])))\n",
    "except:print(\"Model doesn't reach precision of 0.90\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(row, models=[]):\n",
    "    rules = row[\"pred\"]\n",
    "    lr = row[\"lr\"]\n",
    "    roberta_base_mean_hard = row[\"roberta_base_mean_hard\"]\n",
    "    roberta_base_cls_hard = row[\"roberta_base_cls_hard\"]\n",
    "    roberta_large_cls_hard = row[\"roberta_large_cls_hard\"]\n",
    "    bart_large_mean_hard = row[\"bart_large_mean_hard\"]\n",
    "\n",
    "    roberta_base_cls_very_hard = row[\"roberta_base_cls_very_hard\"]\n",
    "    roberta_base_mean_very_hard = row[\"roberta_base_mean_very_hard\"]\n",
    "    roberta_large_cls_very_hard = row[\"roberta_large_cls_very_hard\"]\n",
    "    sentencebert_large_cls_very_hard = row[\"sbert_large_cls_very_hard\"]\n",
    "    sentencebert_large_mean_very_hard = row[\"sbert_large_mean_very_hard\"]\n",
    "    bart_large_mean_very_hard = row[\"bart_large_mean_very_hard\"]\n",
    "\n",
    "    avg = row[\"avg\"]\n",
    "\n",
    "    if \"rules\" in models:\n",
    "        if rules: return 1\n",
    "    if \"lr\" in models:\n",
    "        if lr >= 0.903700: return 1\n",
    "    if \"roberta_base_cls_hard\" in models:\n",
    "        if roberta_base_cls_hard >= 0.988100: return 1\n",
    "    if \"bart_large_mean_hard\" in models:\n",
    "        if bart_large_mean_hard >= 1: return 1\n",
    "    if \"sentencebert_large_mean_very_hard\" in models:\n",
    "        if sentencebert_large_mean_very_hard >= 1: return 1\n",
    "    if \"bart_large_mean_very_hard\" in models:\n",
    "        if bart_large_mean_very_hard >= 1: return 1\n",
    "    if \"avg\" in models:\n",
    "        if avg >= 0.890000: return 1\n",
    "    return 0\n",
    "\n",
    "models = [\n",
    "    \"rules\",\n",
    "    \"lr\",\n",
    "    \"roberta_base_cls_hard\",\n",
    "    \"bart_large_mean_hard\",\n",
    "    \"sentencebert_large_mean_very_hard\",\n",
    "    \"bart_large_mean_very_hard\",\n",
    "    \"avg\"]\n",
    "\n",
    "recalls = []\n",
    "precisions = []\n",
    "for e in range(len(models)):\n",
    "    if e == 0:\n",
    "        model_subset = models[0]\n",
    "    else:\n",
    "        model_subset = models[0:e+1]\n",
    "    y_true = res[\"comp\"]\n",
    "    y_pred = res.apply(lambda x: ensemble(x, model_subset), axis=1)\n",
    "    recall = classification_report(y_true = y_true, y_pred = y_pred, output_dict=True)[\"1\"][\"recall\"]\n",
    "    precision = classification_report(y_true = y_true, y_pred = y_pred, output_dict=True)[\"1\"][\"precision\"]\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_change(recalls):\n",
    "    changes = []\n",
    "    curr = 0\n",
    "    for value in recalls:\n",
    "        change = value - curr\n",
    "        curr = value\n",
    "        changes.append(change)\n",
    "    return changes\n",
    "\n",
    "cumulative_df = pd.DataFrame({\"names\": models, \"precision\": precisions, \"cumulative_recall\":recalls})\n",
    "cumulative_df[\"change\"] = calc_change(cumulative_df[\"cumulative_recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the precision-recall curve for the ensemble classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = models\n",
    "plt.plot(x, recalls, \"bo\")\n",
    "plt.title(\"cumulative recall\")\n",
    "plt.xticks(rotation=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "6b4b7f1d4f62a1961335dfd69b0f36a288a0b1ed2f2ba33eb1a84e0ae1ce9d4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
