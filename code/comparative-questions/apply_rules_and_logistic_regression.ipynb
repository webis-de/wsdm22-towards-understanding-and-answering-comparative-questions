{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to apply the classification rules for classifying questions as comparative or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import ast\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '' # specify the path to the /Webis-CompQuestions-22/comparative-questions-binary-labels/data_full.tsv\n",
    "df_full = pd.read_csv(PATH, sep=\"\\t\")\n",
    "\n",
    "df = df_full.copy()\n",
    "df[\"pos\"] = df[\"pos\"].apply(eval)\n",
    "df[\"lemma\"] = df[\"lemma\"].apply(eval)\n",
    "df[\"tokenized\"] = df[\"clean\"].apply(lambda x: re.split(\" \", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and apply the classification rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_and(inp, regexs):\n",
    "    for rex in regexs:\n",
    "        if rex.match(inp) is None:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def re_or(inp, regexs):\n",
    "    for rex in regexs:\n",
    "        if rex.match(inp) is not None:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def re_one(inp, regex):\n",
    "    if regex.match(inp) is not None:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_interrogative(inp):\n",
    "    interrogratives = [\"who\", \"which\", \"what\", \"where\", \"when\", \"how\", \"why\"]\n",
    "    tokens = word_tokenize(inp[\"clean\"])\n",
    "    for i in interrogratives:\n",
    "        if i in tokens:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_singular_interrogative(inp):\n",
    "    interrogratives = [\"who\", \"which\", \"what\", \"where\"]\n",
    "    tokens = word_tokenize(inp[\"clean\"])\n",
    "    for i in interrogratives:\n",
    "        if i in tokens:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def pos_match(inp, pos_tags):\n",
    "    for p in inp:\n",
    "        if p in pos_tags:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_in_seq(arr, q):\n",
    "    if q[0] not in arr:\n",
    "        return -1\n",
    "    pos = arr.index(q[0])\n",
    "    for it, x in enumerate(q[1:]):\n",
    "        if x != arr[it + pos +1]:\n",
    "            return -1\n",
    "    return pos + len(q)\n",
    "\n",
    "def clean_df():\n",
    "    if \"pred\" in df.columns:\n",
    "        df[\"pred\"] = False\n",
    "    if \"neg_prediction\" in df.columns:\n",
    "        df[\"neg_prediction\"] = False\n",
    "    for col in df.columns:\n",
    "        if \"Unnamed: 0\" in col:\n",
    "            del df[col]\n",
    "        if \"pos_rule\" in col:\n",
    "            del df[col]\n",
    "        if \"neg_rule\" in col:\n",
    "            del df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pred\"] = False\n",
    "df[\"truth\"] = df[\"comp\"].apply(lambda x: True if x==1.0 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex needed for the negative rules (a question is not comparative)\n",
    "\n",
    "rex_neg_rule_0 = re.compile(r'((when|how (long|many|much))\\b.*)')\n",
    "\n",
    "\n",
    "def neg_rule_0(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    regexs = [rex_neg_rule_0]\n",
    "    return re_and(question, regexs)\n",
    "\n",
    "evil_words = [\"lyrics\", \"wrote\", \"mean\", \"covered\", \"cast\", \"played\", \"season\", \"episode\", \"award\", \"sing\", \"sang\", \"song\", \"album\", \"movie\"]\n",
    "\n",
    "def neg_rule_1(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    for evil in evil_words:\n",
    "        if evil in question:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def neg_rule_2(inp):\n",
    "    token = inp['tokenized']\n",
    "    if len(token) < 4:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex needed for the positive rules (a question is comparative)\n",
    "rex_pos_rule_2_1 = re.compile(r'(.*difference between.*)')\n",
    "rex_pos_rule_2_2 = re.compile(r'(.*compare to.*)')\n",
    "rex_pos_rule_3 = re.compile(r'((which) (are|is|\\'s|s)\\b.*\\b(a|an|the)\\b.*)')\n",
    "rex_pos_rule_4_1 = re.compile(r'(what are( some)? good\\b.*)')\n",
    "rex_pos_rule_4_2 = re.compile(r'(.*\\byour favorite\\b.*)')\n",
    "rex_pos_rule_5 = re.compile(r'(.*(who was (the|a|an).*))')\n",
    "rex_pos_rule_6 = re.compile(r'((which).*(should i))')\n",
    "rex_pos_rule_9_1 = re.compile(r'(.*\\b(or|and|from|between|vs|and|versus)\\b.*)')\n",
    "rex_pos_rule_9_2 = re.compile(r'(.*\\b(distinguish|differ|differentiate|differences|strengths|weaknesses)\\b.*)')\n",
    "rex_pos_rule_9_3 = re.compile(r'(.*(the differen(ce|t) between).*)')\n",
    "rex_pos_rule_10 = re.compile(r'(.*(the (best)).*)')\n",
    "rex_pos_rule_11 = re.compile(r'.*\\b(is)\\b.*')\n",
    "rex_pos_rule_13_1 = re.compile(r'(.*are\\b.*and\\b.*same.*)')\n",
    "rex_pos_rule_13_2 = re.compile(r'(.*same.*\\bas\\b.*)')\n",
    "rex_pos_rule_13_3 = re.compile(r'(.*are|is.*)')\n",
    "\n",
    "def pos_rule_1(row):\n",
    "    if row[\"neg_prediction\"]:\n",
    "        return False\n",
    "    tokens = row[\"tokenized\"]\n",
    "    try:\n",
    "        pos_JJ = max(\n",
    "            find_in_seq(tokens, [\"what\", \"is\"]), \n",
    "            find_in_seq(tokens, [\"what\", \"are\"]), \n",
    "            find_in_seq(tokens, [\"what\", \"s\"]),\n",
    "            find_in_seq(tokens, [\"what\", \"'s\"]),\n",
    "            find_in_seq(tokens, [\"which\", \"is\"]), \n",
    "            find_in_seq(tokens, [\"which\", \"are\"]), \n",
    "            find_in_seq(tokens, [\"which\", \"s\"]),\n",
    "            find_in_seq(tokens, [\"which\", \"'s\"]),\n",
    "            find_in_seq(tokens, [\"is\", \"the\"]), \n",
    "            find_in_seq(tokens, [\"are\", \"the\"]), \n",
    "            find_in_seq(tokens, [\"s\", \"the\"]),\n",
    "            find_in_seq(tokens, [\"'s\", \"the\"])\n",
    "\n",
    "        )\n",
    "        if pos_JJ < 0:\n",
    "            return False\n",
    "        if tokens[pos_JJ] in [\"difference\", \"differences\", \"pros\", \"good\", \"advantages\", \"better\", \"similarities\"]:\n",
    "            return True\n",
    "    except:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def pos_rule_2(row):\n",
    "    tokenized = row[\"tokenized\"]\n",
    "    pos = row[\"pos\"]\n",
    "    question = row[\"clean\"]\n",
    "    return re_or(question, [rex_pos_rule_2_1, rex_pos_rule_2_2]) and not row[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_3(row):\n",
    "    question = row[\"clean\"]\n",
    "    pos = row[\"pos\"]\n",
    "    return re_one(question, rex_pos_rule_3) and pos_match(pos, [\"JJS\", \"RBS\", \"JJR\", \"RBR\"]) and not row[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_4(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    return re_or(question, [rex_pos_rule_4_1, rex_pos_rule_4_2]) and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_5(inp):\n",
    "    pos = inp[\"pos\"]\n",
    "    tokens = inp[\"tokenized\"]\n",
    "    question = inp[\"clean\"]\n",
    "    return re_one(question, rex_pos_rule_5) and pos[3] in [\"JJS\", \"RBS\", \"RBR\", \"JJR\"] and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_6(inp):\n",
    "    pos = inp[\"pos\"]\n",
    "    tokens = inp[\"tokenized\"]\n",
    "    question = inp[\"clean\"]\n",
    "    return re_one(question, rex_pos_rule_6) and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_7(inp):\n",
    "    pos = inp[\"pos\"]\n",
    "    tokens = inp[\"tokenized\"]\n",
    "    if pos[0] in [\"JBS\", \"RBS\"]:\n",
    "        return True and not inp[\"neg_prediction\"]\n",
    "    if tokens[0] in [\"the\", \"a\", \"an\"] and pos[1] in [\"JBS\", \"RBS\"]:\n",
    "        return True and not inp[\"neg_prediction\"]\n",
    "    return False\n",
    "\n",
    "def pos_rule_8(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    if inp[\"neg_prediction\"]:\n",
    "        return False\n",
    "    tokens = inp[\"tokenized\"]\n",
    "    try:\n",
    "        pos_JJ = max(\n",
    "            find_in_seq(tokens, [\"what\", \"is\"]), \n",
    "            find_in_seq(tokens, [\"what\", \"are\"]), \n",
    "            find_in_seq(tokens, [\"what\", \"s\"]),\n",
    "            find_in_seq(tokens, [\"what\", \"'s\"]),\n",
    "            find_in_seq(tokens, [\"which\", \"is\"]), \n",
    "            find_in_seq(tokens, [\"which\", \"are\"]), \n",
    "            find_in_seq(tokens, [\"which\", \"s\"]),\n",
    "            find_in_seq(tokens, [\"which\", \"'s\"]),\n",
    "            find_in_seq(tokens, [\"is\", \"the\"]), \n",
    "            find_in_seq(tokens, [\"are\", \"the\"]), \n",
    "            find_in_seq(tokens, [\"s\", \"the\"]),\n",
    "            find_in_seq(tokens, [\"'s\", \"the\"])\n",
    "        )\n",
    "        if pos_JJ < 0:\n",
    "            return False\n",
    "        if pos[pos_JJ] in [\"RBS\", \"JJS\"]:\n",
    "            return True and not neg_rule_2(inp)\n",
    "        if tokens[pos_JJ] in [\"most\"]:\n",
    "            return True and not neg_rule_2(inp)\n",
    "    except:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def pos_rule_9(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    if \"how\" in question:\n",
    "        return False\n",
    "    return re_and(question, [rex_pos_rule_9_1, rex_pos_rule_9_2]) or re_one(question, rex_pos_rule_9_3) and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_10(inp):\n",
    "    pos = inp[\"pos\"]\n",
    "    tokens = inp[\"tokenized\"]\n",
    "    question = inp[\"clean\"]\n",
    "    if ('for the best' in question) or ('how is' in question):\n",
    "        return False\n",
    "    return re_one(question, rex_pos_rule_10) and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_11(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    return has_singular_interrogative(inp) and pos_match(pos, [\"JJS\", \"RBS\"]) and not inp[\"neg_prediction\"]\n",
    "\n",
    "def pos_rule_12(inp):\n",
    "    question = inp[\"clean\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    tokenized = inp[\"tokenized\"]\n",
    "    for it, p in enumerate(pos):\n",
    "        if p in [\"JJS\", \"RBS\"] and \"the\" in tokenized[0: it]:\n",
    "            return True  and not inp[\"neg_prediction\"]\n",
    "    return False\n",
    "\n",
    "def pos_rule_13(inp):\n",
    "    tokenized = inp[\"tokenized\"]\n",
    "    pos = inp[\"pos\"]\n",
    "    question = inp[\"clean\"]\n",
    "    return (re_one(question, rex_pos_rule_13_1) or re_and(question, [rex_pos_rule_13_2, rex_pos_rule_13_3])) and not inp[\"neg_prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df()\n",
    "\n",
    "#NEGATIVE RULES##################################################################\n",
    "#if neg_prediction is True, the question is not a comparative question\n",
    "# if False it is a comparative question\n",
    "neg_rules = [\"neg_rule_0\", \"neg_rule_1\"]\n",
    "df[\"neg_rule_0\"] = df.apply(lambda x: neg_rule_0(x), axis=1)\n",
    "df[\"neg_rule_1\"] = df.apply(lambda x: neg_rule_1(x), axis=1)\n",
    "\n",
    "def make_prediction(old_prediction, prediction_rule):\n",
    "    return old_prediction or prediction_rule\n",
    "\n",
    "df[\"neg_prediction\"] = False\n",
    "for neg_rule in neg_rules:\n",
    "    df[\"neg_prediction\"] = df.apply(lambda x: make_prediction(x[\"neg_prediction\"], x[neg_rule]), axis=1)\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "#POSITIVE RULES###################################################################\n",
    "#if pred is True, question is comparative\n",
    "#if pred is False, question is not comparative\n",
    "pos_rules = [pos_rule_1, pos_rule_2, pos_rule_3, pos_rule_4, pos_rule_5, pos_rule_6, pos_rule_7, pos_rule_8, pos_rule_9, pos_rule_10, pos_rule_11, pos_rule_12, pos_rule_13]\n",
    "pos_rules_100 = [pos_rule_1, pos_rule_2, pos_rule_3, pos_rule_4, pos_rule_5, pos_rule_6, pos_rule_7, pos_rule_8, pos_rule_9, pos_rule_10]\n",
    "\n",
    "precision_per_rule = []\n",
    "recall_per_rule = []\n",
    "precision_total = []\n",
    "recall_total = []\n",
    "for pos_rule in pos_rules_100:\n",
    "    df[pos_rule.__name__] = df.apply(lambda x: pos_rule(x), axis=1)\n",
    "    df[\"pred\"] = df.apply(lambda x: make_prediction(x[\"pred\"], x[pos_rule.__name__]), axis=1)\n",
    "\n",
    "    precision_per_rule.append(precision_score(df[\"truth\"], df[pos_rule.__name__]))\n",
    "    recall_per_rule.append(recall_score(df[\"truth\"], df[pos_rule.__name__]))\n",
    "\n",
    "    precision_total.append(precision_score(df[\"truth\"], df[\"pred\"]))\n",
    "    recall_total.append(recall_score(df[\"truth\"], df[\"pred\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reports the effectiveness of the rules\n",
    "print(classification_report(df[\"truth\"], df[\"pred\"],  digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies and saves questions that are not recognized by the rules as comparative (hard questions)\n",
    "# Hard questions will be used in the logistic regression classifier (next step in the ensemble)\n",
    "\n",
    "OUTPUT_PATH = '' # specifies where to store hard questions, e.g., as a tsv file hard_questions.tsv\n",
    "# For the convienience /Webis-CompQuestions-22/comparative-questions-binary-labels/ already contains hard_questions.tsv\n",
    "# that can be used directly\n",
    "\n",
    "hard_questions = df.loc[np.invert((df[\"comp\"]==1) & (df[\"pred\"]==True))][[\"id\", \"comp\",\"clean\", \"lemma\", \"pos\", \"tokenized\"]]\n",
    "hard_questions.to_csv(OUTPUT_PATH, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression classifier for hard questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, make_scorer, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '' # specifies the path to the hard questions, e.g., as a tsv file hard_questions.tsv\n",
    "# For the convienience /Webis-CompQuestions-22/comparative-questions-binary-labels/ already contains hard_questions.tsv\n",
    "# that can be used directly\n",
    "\n",
    "hard_questions = pd.read_csv(INPUT_PATH, sep=\"\\t\")\n",
    "hard_questions[\"pos\"] = hard_questions[\"pos\"].apply(eval)\n",
    "hard_questions[\"lemma\"] = hard_questions[\"lemma\"].apply(eval)\n",
    "hard_questions[\"tokenized\"] = hard_questions[\"tokenized\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(line):\n",
    "    lemmas = line[\"lemma\"]\n",
    "    pos = line[\"pos\"]\n",
    "    combined = \"\"\n",
    "    for e in range(len(lemmas)):\n",
    "        combined += lemmas[e] + \" \" + pos[e] + \" \"\n",
    "    #combined = \" \".join(lemmas) + \" \"\n",
    "    #combined += \" \".join(pos)\n",
    "    return combined\n",
    "\n",
    "def combine_token_and_pos(line):\n",
    "    tokens = line[\"tokenized\"]\n",
    "    pos = line[\"pos\"]\n",
    "    combined = \"\"\n",
    "    for e in range(len(tokens)):\n",
    "        combined += tokens[e] + \" \" + pos[e] + \" \"\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def strip_punct(s):\n",
    "    s = re.sub('[^А-Яа-яA-Za-z0-9]', ' ', s)\n",
    "    s = s.lower()\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "hard_questions[\"combined\"] = hard_questions.apply(lambda x: combine(x), axis=1)\n",
    "hard_questions[\"lemma\"] = hard_questions[\"lemma\"].apply(lambda x: \" \".join(x))\n",
    "hard_questions = hard_questions.drop_duplicates(subset=\"clean\")\n",
    "hard_questions = hard_questions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = np.array(hard_questions[\"clean\"].tolist())\n",
    "y_train = np.array(hard_questions[\"comp\"].tolist())\n",
    "ids = np.array(hard_questions.id.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with a 10-fold crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = '' # specifies where to store 10 splits with hard questions, e.g., a directory hard_question_splits\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "preds_pos, y_trainCv = list(), list()\n",
    "\n",
    "split = 1\n",
    "very_hard_ids = list()\n",
    "\n",
    "for train_index, test_index in kf.split(train_questions, y_train):\n",
    "    quest_train, quest_test = train_questions[train_index], train_questions[test_index]\n",
    "    y_tr, y_ts = y_train[train_index], y_train[test_index]\n",
    "    hard_ids = ids[test_index]\n",
    "    precision_scores, recall_scores = list(), list()\n",
    "    y_trainCv.append(y_ts)\n",
    "    very_hard_ids.extend(hard_ids)\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(4,4), analyzer='word', min_df = 1, token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "    vectorizer.fit(quest_train)\n",
    "    X_tr = vectorizer.transform(quest_train)\n",
    "    X_ts = vectorizer.transform(quest_test)\n",
    "\n",
    "    clf = LogisticRegression(C = 48., penalty = 'l2', solver = 'liblinear')\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    predictions = clf.predict_proba(X_ts)\n",
    "    binary_predictions = clf.predict(X_ts)\n",
    "\n",
    "    pPred = predictions[:,1]\n",
    "    nPred = predictions[:,0]\n",
    "    predictions = predictions[:,1]    \n",
    "    preds_pos.append(predictions)\n",
    "    \n",
    "    # Saves the 10 train-test splits\n",
    "    questions_clean = hard_questions[hard_questions.clean.isin(quest_test)].clean\n",
    "    \n",
    "    df_out = pd.DataFrame({'comp': y_ts, 'clean': questions_clean, 'neg_prob': nPred, 'pos_prob': pPred})\n",
    "    df_out.to_csv(path_out + 'split_{}'.format(str(split)) + '.tsv', index=False, sep='\\t')\n",
    "    split += 1\n",
    "    \n",
    "    \n",
    "# The code below will plot a precision-reacall curve used for identifying a classifier's operating point\n",
    "\n",
    "preds_pos_flat = np.array([item for sublist in preds_pos for item in sublist])\n",
    "y_trainCv_flat = [item for sublist in y_trainCv for item in sublist]\n",
    "\n",
    "precision_scores, recall_scores = list(), list()\n",
    "\n",
    "# Selecting the probability threshold, for which LR achives a Precision  of 1.0 for comparative questions\n",
    "thresholds = np.arange(1, 0, -0.001)\n",
    "thresholds = np.array(list(np.arange(1, 0.5, -0.0001)) + list(np.arange(0.5, 0, -0.001)))\n",
    "thresholds = [round(p, 6) for p in thresholds]\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    prob_preds = np.where(preds_pos_flat>=threshold, 1, 0)\n",
    "    temp_classification_report = classification_report(y_true=y_trainCv_flat, y_pred=prob_preds, output_dict=True)['1']\n",
    "    precision = round(temp_classification_report['precision'], 3)\n",
    "    precision_scores.append(precision)                      \n",
    "    recall_scores.append(round(temp_classification_report['recall'], 3))\n",
    "    if 0.9 < precision < 0.98:\n",
    "        break\n",
    "\n",
    "l = [item for item in zip(precision_scores, recall_scores, list(thresholds)) if item[0] != 0]\n",
    "\n",
    "plt.plot([i[1] for i in l], [i[0] for i in l], marker='.', label='Logistic')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "l1 = [i for i in zip(precision_scores, recall_scores, list(thresholds)) if i[0]==1]\n",
    "print(\"Comp. questions: max Prec. {:.3f} with Rec. {:.3f} at thresh. {:.6f}\".format(l1[-1][0], l1[-1][1], l1[-1][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save questions that are not classified by the LR as comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "path = '' #specifies where the 10 splits with hard questions were stored, e.g., a directory hard_question_splits (the cell above)\n",
    "dfs = []\n",
    "for file in listdir(path):\n",
    "    df = pd.read_csv(join(path, file), sep='\\t')\n",
    "    dfs.append(df)\n",
    "res_comp = pd.concat(dfs)\n",
    "\n",
    "questions = list()\n",
    "\n",
    "for q in res_comp.clean.tolist():\n",
    "    if res_comp.loc[res_comp.clean == q].pos_prob.values[0] < 0.903700: questions.append(q)\n",
    "\n",
    "train_very_hard = res_comp[res_comp['clean'].isin(questions)]\n",
    "train_very_hard = train_very_hard.drop(['neg_prob', 'pos_prob'], axis=1)\n",
    "\n",
    "PATH_OUT = '' #specifies where to save very hard questions (that are used in the next step of the ensemble), e.g., the file very_hard.tsv\n",
    "\n",
    "train_very_hard.to_csv('PATH_OUT', sep='\\t', index=False)\n",
    "\n",
    "# For the convienience /Webis-CompQuestions-22/comparative-questions-binary-labels/ already contains very_hard.tsv\n",
    "# that can be used directly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
