{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook provides the code to experiment with transformer embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizer, RobertaModel,  LongformerTokenizer, LongformerModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = RobertaModel.from_pretrained('roberta-large')\n",
    "\n",
    "# to experiment with logformer:\n",
    "#tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-large-4096')\n",
    "#model = LongformerModel.from_pretrained('allenai/longformer-large-4096')\n",
    "\n",
    "def extract_features(answers):\n",
    "    '''Used to represent answers using the transformer model defined above'''\n",
    "    X = list()\n",
    "    for answer in tqdm(answers):\n",
    "        input_ids = torch.tensor(tokenizer.encode(answer, max_length=512, truncation=True)).unsqueeze(0)\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0]\n",
    "        X.append(last_hidden_states.detach().numpy())\n",
    "    return X\n",
    "\n",
    "def extract_features_extended(answers, questions):\n",
    "    '''Used to represent question + answer using the transformer model defined above'''\n",
    "    X = list()\n",
    "    for a, q in tqdm(zip(answers, questions)):\n",
    "        answer_concat = q + ' ' + a\n",
    "        input_ids = torch.tensor(tokenizer.encode(answer_concat, max_length=512, truncation=True)).unsqueeze(0)\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0]\n",
    "        X.append(last_hidden_states.detach().numpy())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('DATASET.tsv', sep='\\t', encoding='utf-8') # specify the stance dataset\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2) # create train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answers = np.array(train_df.answer.tolist()) # to experiment with the masked answers use train_df.masked_answer.tolist()\n",
    "train_questions = np.array(train_df.question.tolist()) # alternatevely masked questions can be used\n",
    "y_train = np.array(train_df.answer_stance.tolist())\n",
    "test_answers = np.array(test_df.answer.tolist())   # to experiment with the masked answers use train_df.masked_answer.tolist()\n",
    "test_questions = np.array(test_df.masked_question.tolist()) # alternatevely masked questions can be used\n",
    "y_test = np.array(test_df.answer_stance.tolist())\n",
    "\n",
    "X_train = extract_features(train_answers) # Used for only answer embeddings\n",
    "#X_train = extract_features_extended(train_answers, train_questions) # used for the embeddings question + answer\n",
    "X_test = extract_features(test_answers)\n",
    "#X_test = extract_features_extended(test_answers, test_questions)\n",
    "\n",
    "X_train_mean = np.array([np.mean(X[0], axis=0) for X in X_train]) # mean of all token embeddings\n",
    "X_train_cls = np.array([X[0][0] for X in X_train]) # only the CLS-token embedding\n",
    "X_test_mean = np.array([np.mean(X[0], axis=0) for X in X_test])\n",
    "X_test_cls = np.array([X[0][0] for X in X_test])\n",
    "\n",
    "del X_train\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Find the optimal hyper-parameters for logistic regression\n",
    "\n",
    "tuned_parameters = {'penalty' : ['l1', 'l2'],\n",
    "                    'C' : np.logspace(-4, 4, 20),\n",
    "                    'solver' : ['liblinear', 'lbfgs']}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "clf = GridSearchCV(lr, tuned_parameters, cv=5, scoring='accuracy')\n",
    "clf.fit(X_train_mean, y_train) # for only CLS-token embeddings use X_train_cls\n",
    "print(\"Best parameters set found on the train set:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf = LogisticRegression(C=0.07, penalty='l2', solver='lbfgs')\n",
    "#clf = LinearSVC() # to use the SVM classifier \n",
    "\n",
    "clf.fit(X_train_mean, y_train) # for only CLS-token embeddings use X_train_cls\n",
    "y_pred = clf.predict(X_test_mean)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=3, zero_division=False))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import TruePositives, Precision\n",
    "from keras.utils import np_utils\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(768, input_shape=(1024,), activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='loss', mode='min', verbose=0)\n",
    "\n",
    "Y = np_utils.to_categorical(y_train)\n",
    "model.fit(np.array(X_train_mean), Y, epochs=100, batch_size=5,verbose=0, callbacks=[es]) # for only CLS-token embeddings use X_train_cls\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test_mean), axis=-1)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=3, zero_division=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
